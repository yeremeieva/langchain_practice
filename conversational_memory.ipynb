{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conversation Chain",
   "id": "8d9f11e9b70956e2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-19T19:48:07.175926Z",
     "start_time": "2024-04-19T19:48:07.163573Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import (ConversationBufferMemory,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  ConversationSummaryMemory,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  ConversationBufferWindowMemory,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  ConversationKGMemory)\n",
    "import os\n",
    "from langchain.callbacks import get_openai_callback"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:34:03.540078Z",
     "start_time": "2024-04-19T19:34:03.537915Z"
    }
   },
   "cell_type": "code",
   "source": "os.environ['OPENAI_API_KEY'] = \"\"",
   "id": "e6ef54b447e04a36",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:34:04.078977Z",
     "start_time": "2024-04-19T19:34:04.066605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI(\n",
    "\ttemperature=0,\n",
    "\tmodel_name=\"gpt-3.5-turbo\"\n",
    ")"
   ],
   "id": "6d5ea0c1b48dddb0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:45:05.892359Z",
     "start_time": "2024-04-19T19:45:05.888753Z"
    }
   },
   "cell_type": "code",
   "source": "conversation = ConversationChain(llm=llm)",
   "id": "f3342a14aeb85480",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:35:23.105429Z",
     "start_time": "2024-04-19T19:35:23.101214Z"
    }
   },
   "cell_type": "code",
   "source": "print(conversation.prompt.template)",
   "id": "a21d9b0526de3da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:51:09.822361Z",
     "start_time": "2024-04-19T19:51:09.818466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.invoke(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "    return result"
   ],
   "id": "63e62c8ffa43382d",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ConversationBufferMemory",
   "id": "e3ffaa4701b4bd32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:51:23.427155Z",
     "start_time": "2024-04-19T19:51:23.424457Z"
    }
   },
   "cell_type": "code",
   "source": "conversation_buf = ConversationChain(llm=llm, memory=ConversationBufferMemory())",
   "id": "fe975e3b1b79269f",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:51:25.051663Z",
     "start_time": "2024-04-19T19:51:24.302542Z"
    }
   },
   "cell_type": "code",
   "source": "conversation_buf.invoke(\"Good morning AI!\")",
   "id": "412bcdc3f9f216bb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': '',\n",
       " 'response': 'Good morning! How are you today?'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:51:28.120074Z",
     "start_time": "2024-04-19T19:51:25.925896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ],
   "id": "a86a398a7070ac83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 172 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'My interest here is to explore the potential of integrating Large Language Models with external knowledge',\n",
       " 'history': 'Human: Good morning AI!\\nAI: Good morning! How are you today?',\n",
       " 'response': \"That's a fascinating topic! Large Language Models like GPT-3 have shown great potential in generating human-like text, but integrating them with external knowledge sources could enhance their capabilities even further. By connecting these models to databases, websites, or other sources of information, they could provide more accurate and contextually relevant responses. Have you considered any specific approaches or tools for this integration?\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:56:31.066431Z",
     "start_time": "2024-04-19T19:56:27.629608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation_buf,\n",
    "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
    ")"
   ],
   "id": "f1e3e946c2040d6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 313 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'I just want to analyze the different possibilities. What can you think of?',\n",
       " 'history': \"Human: Good morning AI!\\nAI: Good morning! How are you today?\\nHuman: My interest here is to explore the potential of integrating Large Language Models with external knowledge\\nAI: That's a fascinating topic! Large Language Models like GPT-3 have shown great potential in generating human-like text, but integrating them with external knowledge sources could enhance their capabilities even further. By connecting these models to databases, websites, or other sources of information, they could provide more accurate and contextually relevant responses. Have you considered any specific approaches or tools for this integration?\",\n",
       " 'response': 'There are several approaches you could consider for integrating Large Language Models with external knowledge. One option is to use knowledge graphs, which organize information in a structured way that can be easily accessed by the model. Another approach is to use APIs to connect the model to external databases or websites, allowing it to retrieve real-time information. Additionally, you could explore the use of pre-trained models that have already been fine-tuned on specific knowledge domains. These are just a few possibilities to consider, and each has its own advantages and challenges. Do any of these options resonate with you, or are you looking for something different?'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:57:40.822524Z",
     "start_time": "2024-04-19T19:57:36.541215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"Which data source types could be used to give context to the model?\"\n",
    ")"
   ],
   "id": "45a520e39faabbe6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 510 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Which data source types could be used to give context to the model?',\n",
       " 'history': \"Human: Good morning AI!\\nAI: Good morning! How are you today?\\nHuman: My interest here is to explore the potential of integrating Large Language Models with external knowledge\\nAI: That's a fascinating topic! Large Language Models like GPT-3 have shown great potential in generating human-like text, but integrating them with external knowledge sources could enhance their capabilities even further. By connecting these models to databases, websites, or other sources of information, they could provide more accurate and contextually relevant responses. Have you considered any specific approaches or tools for this integration?\\nHuman: I just want to analyze the different possibilities. What can you think of?\\nAI: There are several approaches you could consider for integrating Large Language Models with external knowledge. One option is to use knowledge graphs, which organize information in a structured way that can be easily accessed by the model. Another approach is to use APIs to connect the model to external databases or websites, allowing it to retrieve real-time information. Additionally, you could explore the use of pre-trained models that have already been fine-tuned on specific knowledge domains. These are just a few possibilities to consider, and each has its own advantages and challenges. Do any of these options resonate with you, or are you looking for something different?\",\n",
       " 'response': \"There are various types of data sources that could be used to give context to the model. Some common examples include structured databases, unstructured text documents, knowledge graphs, APIs, and even real-time web scraping. Structured databases provide organized information in a tabular format, making it easy for the model to access and understand. Unstructured text documents, on the other hand, can provide a wealth of textual information that can be used to enrich the model's understanding of a particular topic. Knowledge graphs offer a way to represent information in a graph format, showing relationships between different entities. APIs allow the model to interact with external systems and retrieve specific data points. Real-time web scraping can provide up-to-date information from websites, news articles, or social media platforms. By combining these different data source types, you can give the model a rich context to work with and generate more accurate and relevant responses.\"}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:57:49.257894Z",
     "start_time": "2024-04-19T19:57:47.680129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"What is my aim again?\"\n",
    ")"
   ],
   "id": "6a85be4e3ec19b92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 581 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my aim again?',\n",
       " 'history': \"Human: Good morning AI!\\nAI: Good morning! How are you today?\\nHuman: My interest here is to explore the potential of integrating Large Language Models with external knowledge\\nAI: That's a fascinating topic! Large Language Models like GPT-3 have shown great potential in generating human-like text, but integrating them with external knowledge sources could enhance their capabilities even further. By connecting these models to databases, websites, or other sources of information, they could provide more accurate and contextually relevant responses. Have you considered any specific approaches or tools for this integration?\\nHuman: I just want to analyze the different possibilities. What can you think of?\\nAI: There are several approaches you could consider for integrating Large Language Models with external knowledge. One option is to use knowledge graphs, which organize information in a structured way that can be easily accessed by the model. Another approach is to use APIs to connect the model to external databases or websites, allowing it to retrieve real-time information. Additionally, you could explore the use of pre-trained models that have already been fine-tuned on specific knowledge domains. These are just a few possibilities to consider, and each has its own advantages and challenges. Do any of these options resonate with you, or are you looking for something different?\\nHuman: Which data source types could be used to give context to the model?\\nAI: There are various types of data sources that could be used to give context to the model. Some common examples include structured databases, unstructured text documents, knowledge graphs, APIs, and even real-time web scraping. Structured databases provide organized information in a tabular format, making it easy for the model to access and understand. Unstructured text documents, on the other hand, can provide a wealth of textual information that can be used to enrich the model's understanding of a particular topic. Knowledge graphs offer a way to represent information in a graph format, showing relationships between different entities. APIs allow the model to interact with external systems and retrieve specific data points. Real-time web scraping can provide up-to-date information from websites, news articles, or social media platforms. By combining these different data source types, you can give the model a rich context to work with and generate more accurate and relevant responses.\",\n",
       " 'response': 'Your aim is to explore the potential of integrating Large Language Models with external knowledge sources to enhance their capabilities and generate more accurate and contextually relevant responses. By analyzing different possibilities and considering various approaches and data source types, you are seeking to improve the performance and effectiveness of these models in generating human-like text.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T21:48:36.702454Z",
     "start_time": "2024-04-19T21:48:36.696899Z"
    }
   },
   "cell_type": "code",
   "source": "print(conversation_buf.memory.buffer)",
   "id": "551b5f289ece11bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Good morning AI!\n",
      "AI: Good morning! How are you today?\n",
      "Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "AI: That's a fascinating topic! Large Language Models like GPT-3 have shown great potential in generating human-like text, but integrating them with external knowledge sources could enhance their capabilities even further. By connecting these models to databases, websites, or other sources of information, they could provide more accurate and contextually relevant responses. Have you considered any specific approaches or tools for this integration?\n",
      "Human: I just want to analyze the different possibilities. What can you think of?\n",
      "AI: There are several approaches you could consider for integrating Large Language Models with external knowledge. One option is to use knowledge graphs, which organize information in a structured way that can be easily accessed by the model. Another approach is to use APIs to connect the model to external databases or websites, allowing it to retrieve real-time information. Additionally, you could explore the use of pre-trained models that have already been fine-tuned on specific knowledge domains. These are just a few possibilities to consider, and each has its own advantages and challenges. Do any of these options resonate with you, or are you looking for something different?\n",
      "Human: Which data source types could be used to give context to the model?\n",
      "AI: There are various types of data sources that could be used to give context to the model. Some common examples include structured databases, unstructured text documents, knowledge graphs, APIs, and even real-time web scraping. Structured databases provide organized information in a tabular format, making it easy for the model to access and understand. Unstructured text documents, on the other hand, can provide a wealth of textual information that can be used to enrich the model's understanding of a particular topic. Knowledge graphs offer a way to represent information in a graph format, showing relationships between different entities. APIs allow the model to interact with external systems and retrieve specific data points. Real-time web scraping can provide up-to-date information from websites, news articles, or social media platforms. By combining these different data source types, you can give the model a rich context to work with and generate more accurate and relevant responses.\n",
      "Human: What is my aim again?\n",
      "AI: Your aim is to explore the potential of integrating Large Language Models with external knowledge sources to enhance their capabilities and generate more accurate and contextually relevant responses. By analyzing different possibilities and considering various approaches and data source types, you are seeking to improve the performance and effectiveness of these models in generating human-like text.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T21:57:03.511483Z",
     "start_time": "2024-04-19T21:57:03.505301Z"
    }
   },
   "cell_type": "code",
   "source": "conversation_sum = ConversationChain(llm=llm, memory=ConversationSummaryMemory(llm=llm))",
   "id": "2b35730b7f9d6510",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T21:58:16.188810Z",
     "start_time": "2024-04-19T21:58:16.186596Z"
    }
   },
   "cell_type": "code",
   "source": "print(conversation_sum.memory.prompt.template)",
   "id": "1572ab980c4fa4a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T22:00:07.130016Z",
     "start_time": "2024-04-19T22:00:05.357317Z"
    }
   },
   "cell_type": "code",
   "source": "count_tokens(conversation_sum, \"Good morning AI!\")",
   "id": "8125ee6210c4cea9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 251 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': '',\n",
       " 'response': 'Good morning! How are you today?'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T22:01:19.219071Z",
     "start_time": "2024-04-19T22:01:14.995953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ],
   "id": "82a7f7c006f296ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 574 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'My interest here is to explore the potential of integrating Large Language Models with external knowledge',\n",
       " 'history': 'The human greets the AI with a \"Good morning.\" The AI responds in kind and asks how the human is feeling today.',\n",
       " 'response': \"That's a fascinating topic! Large Language Models like GPT-3 have shown great potential in generating human-like text, but integrating them with external knowledge sources could take their capabilities to the next level. There are various approaches to incorporating external knowledge, such as knowledge graphs, databases, or even real-time web scraping. It's an exciting area of research with a lot of possibilities. Do you have any specific ideas or goals in mind for this integration?\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T22:01:25.376385Z",
     "start_time": "2024-04-19T22:01:19.221179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
    ")"
   ],
   "id": "62e868f003296724",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 884 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'I just want to analyze the different possibilities. What can you think of?',\n",
       " 'history': 'The human greets the AI with a \"Good morning.\" The AI responds in kind and asks how the human is feeling today. The human expresses interest in exploring the potential of integrating Large Language Models with external knowledge. The AI finds this topic fascinating and discusses how integrating external knowledge sources with models like GPT-3 could enhance their capabilities. The AI mentions various approaches to incorporating external knowledge and highlights the exciting possibilities in this area of research. The AI asks the human if they have any specific ideas or goals in mind for this integration.',\n",
       " 'response': \"There are several ways to integrate external knowledge with Large Language Models like GPT-3. One approach is to use knowledge graphs, which organize information in a structured format that can be easily accessed by the model. Another approach is to use pre-trained embeddings that capture the relationships between words and concepts in a way that can be used to enhance the model's understanding of the world. Additionally, models can be fine-tuned on specific knowledge domains to improve their performance on tasks related to that domain. These are just a few examples of how external knowledge can be integrated with Large Language Models to enhance their capabilities. Do you have any specific goals or ideas in mind for this integration?\"}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T22:01:31.505897Z",
     "start_time": "2024-04-19T22:01:25.378476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Which data source types could be used to give context to the model?\"\n",
    ")"
   ],
   "id": "3db8e2bd80e3a2d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 986 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Which data source types could be used to give context to the model?',\n",
       " 'history': 'The human greets the AI with a \"Good morning.\" The AI responds in kind and asks how the human is feeling today. The human expresses interest in exploring the potential of integrating Large Language Models with external knowledge. The AI finds this topic fascinating and discusses how integrating external knowledge sources with models like GPT-3 could enhance their capabilities. The AI mentions various approaches to incorporating external knowledge and highlights the exciting possibilities in this area of research. The AI asks the human if they have any specific ideas or goals in mind for this integration. The human expresses a desire to analyze the different possibilities, prompting the AI to discuss various ways to integrate external knowledge with Large Language Models, such as using knowledge graphs, pre-trained embeddings, and domain-specific fine-tuning. The AI then asks the human if they have any specific goals or ideas in mind for this integration.',\n",
       " 'response': 'There are several types of data sources that could be used to give context to the model, such as structured databases, unstructured text documents, knowledge graphs, pre-trained embeddings, domain-specific datasets, and even real-time data feeds. Each type of data source has its own strengths and can provide valuable context to enhance the capabilities of Large Language Models like GPT-3. Would you like me to provide more details on any specific type of data source?'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T22:01:35.503618Z",
     "start_time": "2024-04-19T22:01:31.508568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"What is my aim again?\"\n",
    ")"
   ],
   "id": "3c553c52720fcbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 881 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my aim again?',\n",
       " 'history': 'The human greets the AI with a \"Good morning.\" The AI responds in kind and asks how the human is feeling today. The human expresses interest in exploring the potential of integrating Large Language Models with external knowledge. The AI finds this topic fascinating and discusses how integrating external knowledge sources with models like GPT-3 could enhance their capabilities. The AI mentions various approaches to incorporating external knowledge and highlights the exciting possibilities in this area of research. The AI asks the human if they have any specific ideas or goals in mind for this integration. The human expresses a desire to analyze the different possibilities, prompting the AI to discuss various ways to integrate external knowledge with Large Language Models, such as using knowledge graphs, pre-trained embeddings, and domain-specific fine-tuning. The AI then asks the human if they have any specific goals or ideas in mind for this integration. The human inquires about the types of data sources that could be used to give context to the model. The AI explains that structured databases, unstructured text documents, knowledge graphs, pre-trained embeddings, domain-specific datasets, and real-time data feeds are all potential data sources that could enhance the capabilities of Large Language Models.',\n",
       " 'response': 'Your aim is to explore the potential of integrating Large Language Models with external knowledge to enhance their capabilities.'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T22:03:13.423317Z",
     "start_time": "2024-04-19T22:03:13.420771Z"
    }
   },
   "cell_type": "code",
   "source": "print(conversation_sum.memory.buffer)",
   "id": "2fa3b0c167d554b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human greets the AI with a \"Good morning\" and the AI responds in kind, asking how the human is feeling. The conversation then delves into the fascinating topic of integrating external knowledge with Large Language Models like GPT-3 to enhance their capabilities. Various approaches and possibilities are discussed, with the human expressing interest in analyzing different possibilities. The AI explains ways to integrate external knowledge, such as using knowledge graphs and domain-specific fine-tuning. The human inquires about data sources for context, and the AI lists structured databases, text documents, knowledge graphs, pre-trained embeddings, domain-specific datasets, and real-time data feeds as potential sources. The human then asks about their aim, which is to explore the potential of integrating Large Language Models with external knowledge to enhance their capabilities.\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:07:30.269205Z",
     "start_time": "2024-04-19T23:07:30.265374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversation_bufw = ConversationChain(\n",
    "\tllm=llm,\n",
    "\tmemory=ConversationBufferWindowMemory(k=1)\n",
    ")"
   ],
   "id": "be08e7cce039c832",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:07:31.735171Z",
     "start_time": "2024-04-19T23:07:31.155314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Good morning AI!\"\n",
    ")"
   ],
   "id": "ee24e6eb029d8b85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 75 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': '',\n",
       " 'response': 'Good morning! How are you today?'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:07:41.443374Z",
     "start_time": "2024-04-19T23:07:39.598178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ],
   "id": "1254b9675c8b0c6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 189 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'My interest here is to explore the potential of integrating Large Language Models with external knowledge',\n",
       " 'history': 'Human: Good morning AI!\\nAI: Good morning! How are you today?',\n",
       " 'response': \"That's a fascinating topic! Large Language Models like GPT-3 have shown great potential in generating human-like text, but integrating them with external knowledge sources could greatly enhance their capabilities. There are already efforts underway to connect these models with databases, websites, and other sources of information to improve their understanding and accuracy. It's an exciting area of research with a lot of potential for innovation and advancement. What specific aspects of this integration are you interested in exploring further?\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:07:49.230125Z",
     "start_time": "2024-04-19T23:07:46.974052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
    ")"
   ],
   "id": "18d3900801d7ee3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 309 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'I just want to analyze the different possibilities. What can you think of?',\n",
       " 'history': \"Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\\nAI: That's a fascinating topic! Large Language Models like GPT-3 have shown great potential in generating human-like text, but integrating them with external knowledge sources could greatly enhance their capabilities. There are already efforts underway to connect these models with databases, websites, and other sources of information to improve their understanding and accuracy. It's an exciting area of research with a lot of potential for innovation and advancement. What specific aspects of this integration are you interested in exploring further?\",\n",
       " 'response': \"There are several possibilities for integrating Large Language Models with external knowledge sources. One option is to use structured data from databases or knowledge graphs to provide context and improve the accuracy of the model's responses. Another approach is to leverage information from websites or online sources to enhance the model's understanding of specific topics or domains. Additionally, integrating real-time data feeds could help keep the model up-to-date and relevant in dynamic environments. These are just a few examples, and there are likely many more potential avenues to explore in this area. What specific use cases or applications are you considering for this integration?\"}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:07:55.779677Z",
     "start_time": "2024-04-19T23:07:53.981038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Which data source types could be used to give context to the model?\"\n",
    ")"
   ],
   "id": "dddd8c70639968c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 313 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Which data source types could be used to give context to the model?',\n",
       " 'history': \"Human: I just want to analyze the different possibilities. What can you think of?\\nAI: There are several possibilities for integrating Large Language Models with external knowledge sources. One option is to use structured data from databases or knowledge graphs to provide context and improve the accuracy of the model's responses. Another approach is to leverage information from websites or online sources to enhance the model's understanding of specific topics or domains. Additionally, integrating real-time data feeds could help keep the model up-to-date and relevant in dynamic environments. These are just a few examples, and there are likely many more potential avenues to explore in this area. What specific use cases or applications are you considering for this integration?\",\n",
       " 'response': \"Some common data source types that could be used to give context to a Large Language Model include structured databases, knowledge graphs, websites, online sources, real-time data feeds, text corpora, and domain-specific datasets. These sources can provide valuable information to help the model better understand and generate accurate responses in various contexts. Additionally, incorporating user feedback and interactions can also contribute to improving the model's performance and relevance. Is there a specific type of data source you are interested in exploring further for context integration?\"}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:08:19.437978Z",
     "start_time": "2024-04-19T23:08:16.344564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"What is my aim again?\"\n",
    ")"
   ],
   "id": "3df3f3894871263f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 307 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my aim again?',\n",
       " 'history': \"Human: Which data source types could be used to give context to the model?\\nAI: Some common data source types that could be used to give context to a Large Language Model include structured databases, knowledge graphs, websites, online sources, real-time data feeds, text corpora, and domain-specific datasets. These sources can provide valuable information to help the model better understand and generate accurate responses in various contexts. Additionally, incorporating user feedback and interactions can also contribute to improving the model's performance and relevance. Is there a specific type of data source you are interested in exploring further for context integration?\",\n",
       " 'response': \"Your aim is to understand how different data source types can be used to provide context to a model, specifically a Large Language Model. By exploring the various data sources mentioned earlier, you can gain insights into how to enhance the model's performance and accuracy by incorporating relevant information from structured databases, knowledge graphs, websites, online sources, real-time data feeds, text corpora, domain-specific datasets, user feedback, and interactions. This understanding can help you optimize the model's capabilities and generate more precise responses in different contexts. Let me know if you have any specific questions or need further clarification on this topic.\"}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:08:32.253619Z",
     "start_time": "2024-04-19T23:08:32.250556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bufw_history = conversation_bufw.memory.load_memory_variables(\n",
    "    inputs=[]\n",
    ")['history']"
   ],
   "id": "489ea6584b7d0f55",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T23:08:39.383233Z",
     "start_time": "2024-04-19T23:08:39.380420Z"
    }
   },
   "cell_type": "code",
   "source": "print(bufw_history)",
   "id": "af21247dc2437c66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my aim again?\n",
      "AI: Your aim is to understand how different data source types can be used to provide context to a model, specifically a Large Language Model. By exploring the various data sources mentioned earlier, you can gain insights into how to enhance the model's performance and accuracy by incorporating relevant information from structured databases, knowledge graphs, websites, online sources, real-time data feeds, text corpora, domain-specific datasets, user feedback, and interactions. This understanding can help you optimize the model's capabilities and generate more precise responses in different contexts. Let me know if you have any specific questions or need further clarification on this topic.\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conversation_sum_bufw = ConversationChain(\n",
    "    llm=llm, memory=ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        max_token_limit=650\n",
    "))"
   ],
   "id": "92cbf16eae3b0a00"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
